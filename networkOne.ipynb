{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network One: classifier for music vs. non-music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the first part of the assigment for ECS7013P, Deep Learning for Audio and Music, QMUL.  \n",
    "It consists of an **implementation of a neural network for binary classification between music and non-music from audio samples**.  \n",
    "Code and ideas were based, built upon and inspired by:  \n",
    "- Lecture 3 from Deep Learning for Audio and Music (ECS7013P) on pre-trained features, QMUL, by Dan Stowell.  \n",
    "- https://github.com/slychief/ismir2018_tutorial \n",
    "- https://github.com/keunwoochoi/dl4mir  \n",
    "- https://github.com/tensorflow/models/tree/master/research/audioset/vggish\n",
    "\n",
    "Also, this implementation uses pre-trained features, exploring torchvggish's package available at:  \n",
    "- https://github.com/harritaylor/torchvggish  \n",
    "\n",
    "Training and testing were conducted using GTZAN and URBANSOUND8K datasets available at:  \n",
    "- [GTZAN] http://marsyas.info/downloads/datasets.html  \n",
    "- [URBANSOUND8K] https://urbansounddataset.weebly.com/urbansound8k.html  \n",
    "\n",
    "As per this coursework's requirements, a test case audio file is needed to evaluate the network's performance. A track from *Conduct*, by October Horse, a progressive metal band from Porto, Portugal was chosen:  \n",
    "- https://octoberhorse.bandcamp.com/track/waving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "import torchvggish\n",
    "from torchvggish import vggish, vggish_input\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to where the datasets (GTZAN and URBANSOUND8K) are stored on local filesystem\n",
    "path_to_gtzan  = \"/import/c4dm-datasets/gtzan/\"\n",
    "path_to_urban = \"/import/c4dm-datasets/UrbanSound8K/audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(PATH1, PATH2, return_all=True):\n",
    "    '''\n",
    "    Function to create dictionaries from datasets.\n",
    "    Arguments:\n",
    "        PATH1: path to urban sound 8k dataset\n",
    "        PATH2: path to gtzan dataset\n",
    "        return_all: if true returns 3 dictionaries, one for each dataset and one combined\n",
    "    Return:\n",
    "        Dictionaries with file path and label (non-music/music)\n",
    "    '''\n",
    "    \n",
    "    path_to_urban = PATH1\n",
    "    path_to_gtzan = PATH2\n",
    "    \n",
    "    # Limit UrbanSound dataset to balance both datasets\n",
    "    maxfilestoload  = 120  \n",
    "    \n",
    "    # UrbanSound Dataset\n",
    "    # create dictionary with file name as key \n",
    "    # and the corresponding label (0, no music)\n",
    "    urbanfiles_Dictionary ={}\n",
    "\n",
    "    for entry in os.scandir(path_to_urban):\n",
    "            if os.path.isdir(entry): \n",
    "                counter = 0\n",
    "                for wavFile in os.scandir(entry.path):\n",
    "                    counter = counter + 1\n",
    "                    if (counter <= maxfilestoload): \n",
    "                        filename = wavFile.name\n",
    "                        filename = filename[:-4]\n",
    "                        # Label it as zero, for non-music\n",
    "                        urbanfiles_Dictionary[entry.name+\"/\"+filename] = 0\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "    ## GTZAN dataset\n",
    "    # create dictionary with file name as key \n",
    "    # and the corresponding label (1, music)\n",
    "    gtzanfiles_Dictionary ={}\n",
    "    for entry in os.scandir(path_to_gtzan):\n",
    "            if os.path.isdir(entry): \n",
    "                for wavFile in os.scandir(entry.path):\n",
    "                    filename = wavFile.name\n",
    "                    filename = filename[:-3] \n",
    "                    # Label it as one, for music\n",
    "                    gtzanfiles_Dictionary[entry.name+\"/\"+filename] = 1\n",
    "    \n",
    "    # Combine label dictionaries into one\n",
    "    files_Dictionary = {}\n",
    "    files_Dictionary.update(urbanfiles_Dictionary)\n",
    "    files_Dictionary.update(gtzanfiles_Dictionary)\n",
    "    \n",
    "    if return_all:\n",
    "        return files_Dictionary, urbanfiles_Dictionary, gtzanfiles_Dictionary\n",
    "    else:\n",
    "        return files_Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries\n",
    "files_Dictionary, urbanfiles_Dictionary, gtzanfiles_Dictionary = create_dictionaries(path_to_urban, path_to_gtzan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vggish mel spectograms\n",
    "# Details about procedure and dimensions can be found at:\n",
    "#    https://github.com/tensorflow/models/tree/master/research/audioset/vggish\n",
    "urban_melspecs_vgg = {key:vggish_input.waveform_to_examples(librosa.load(\"%s/%s.wav\" % (path_to_urban, key))[0], librosa.load(\"%s/%s.wav\" % (path_to_urban, key))[1])\n",
    "for key, label in urbanfiles_Dictionary.items() }\n",
    "gtzan_melspecs_vgg = {key:vggish_input.waveform_to_examples(librosa.load(\"%s/%s.au\" % (path_to_gtzan, key), duration=4, offset=10)[0], librosa.load(\"%s/%s.au\" % (path_to_gtzan, key), duration=4, offset=10)[1])\n",
    "for key, label in gtzanfiles_Dictionary.items() }\n",
    "\n",
    "# Merge mel spectograms into one single dictionary\n",
    "melspecs = {}\n",
    "melspecs.update(urban_melspecs_vgg)\n",
    "melspecs.update(gtzan_melspecs_vgg)\n",
    "\n",
    "# Save mel spectrograms with pickle \n",
    "melspecs_pickle = open(PATH + '/' + 'melspecs.pickle',\"wb\")\n",
    "pickle.dump(melspecs, melspecs_pickle)\n",
    "melspecs_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrograms(path):\n",
    "    '''\n",
    "    Function to load spectograms from pickle.\n",
    "    '''\n",
    "    with open(path+ '/' + 'melspecs.pickle', 'rb') as handle:\n",
    "        melspecs = pickle.load(handle)\n",
    "    return melspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mel spectograms\n",
    "melspecs = load_spectrograms(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both melspecs and label dictionaries, delete entries withshort audio (present in UrbanSound8K)\n",
    "for itemid in list(melspecs.keys()):\n",
    "    if melspecs[itemid].shape[0] == 0:\n",
    "            del melspecs[itemid]\n",
    "            del urbanfiles_Dictionary[itemid]\n",
    "            del files_Dictionary[itemid]\n",
    "            \n",
    "# To make melspecs with 1000 examples from each dataset            \n",
    "for itemid in list(urbanfiles_Dictionary.keys()):\n",
    "    if len(melspecs)>2000:\n",
    "        del melspecs[itemid]\n",
    "        del urbanfiles_Dictionary[itemid]\n",
    "        del files_Dictionary[itemid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dictionary with file names\n",
    "keys = list(files_Dictionary.keys())\n",
    "np.random.shuffle(keys)\n",
    "\n",
    "# Shuffle melspecs with the new keys\n",
    "melfeatures_Dict = {}\n",
    "for key in keys:\n",
    "    melfeatures_Dict.update({key:melspecs[key]})\n",
    "    \n",
    "del melspecs\n",
    "melspecs = melfeatures_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT CELL TO SEE/HEAR ONE EXAMPLE FROM EACH DATASET\n",
    "\n",
    "# urban_key = 'fold8/156358-5-0-4'   # Engine idling\n",
    "# gtzan_key = 'metal/metal.00043'    # Short snippet of the amazing Ronnie James Dio\n",
    "\n",
    "# urban_example_4D = melspecs[urban_key].detach().numpy()\n",
    "# # Use numpy \"concatenate\" to join the 1-second chunks back together.\n",
    "# urban_example_2D = np.concatenate(np.concatenate(urban_example_4D)).T\n",
    "\n",
    "# gtzan_example_4D = melspecs[gtzan_key].detach().numpy()\n",
    "# gtzan_example_2D = np.concatenate(np.concatenate(gtzan_example_4D)).T\n",
    "\n",
    "# # Plotting side-by-side to visually inspect mel spectograms\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "# axes[0].imshow(urban_example_2D, aspect='auto', origin='lower')\n",
    "# axes[0].set_title('UrbanSound8k: ' + urban_key)\n",
    "# axes[0].set_ylabel('Mel bins')\n",
    "# axes[0].set_xlabel('Time frames')\n",
    "# axes[1].imshow(gtzan_example_2D, aspect='auto', origin='lower')\n",
    "# axes[1].set_title('GTZAN: ' + gtzan_key)\n",
    "# axes[1].set_ylabel('Mel bins')\n",
    "# axes[1].set_xlabel('Time frames')\n",
    "# fig.tight_layout()\n",
    "\n",
    "# # Aurally inspect the files\n",
    "# print('UrbanSound8k: ' + urban_key)\n",
    "# ipd.display(ipd.Audio(path_to_urban + urban_key + \".wav\"))\n",
    "\n",
    "# print('GTZAN: ' + gtzan_key)\n",
    "# ipd.display(ipd.Audio(librosa.load(\"%s%s.au\" % (path_to_gtzan, gtzan_key), duration=4, offset=10)[0], \n",
    "#           rate=librosa.load(\"%s%s.au\" % (path_to_gtzan, gtzan_key), duration=4, offset=10)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After shuffling, split dataset into (70%) training and (30%) test \n",
    "melspecs_test = {}\n",
    "melspecs_train = {}\n",
    "split = 0.7\n",
    "for i, key in enumerate(melspecs.keys()):\n",
    "    if (i >= (split*len(melspecs))):\n",
    "        melspecs_test[key]  = melspecs[key]\n",
    "    else:\n",
    "        melspecs_train[key] = melspecs[key]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings, passing data through the VGGish network\n",
    "# Details about procedure and dimensions can be found at:\n",
    "#    https://github.com/tensorflow/models/tree/master/research/audioset/vggish\n",
    "embedding_model = vggish()\n",
    "embeddings = {key:embedding_model.forward(item).detach().numpy() for key,item in sorted(melspecs.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for training set\n",
    "binarylabels_serial_train = []\n",
    "Xforpca_vgg_train = []\n",
    "keys = []\n",
    "for akey in melspecs_train.keys():\n",
    "        if akey in embeddings:\n",
    "                if embeddings[akey].shape != (4,128):\n",
    "                        #print(\"  warning: skipping item %s because embedding is unexpected shape\" % akey)\n",
    "                        continue # goes to the beginning of the loop\n",
    "                binarylabels_serial_train.extend([files_Dictionary[akey]] * embeddings[akey].shape[0])\n",
    "                #print(embeddings[akey].shape[1])\n",
    "                Xforpca_vgg_train.append(embeddings[akey])\n",
    "                #print(Xforpca_vgg[0].shape)\n",
    "                keys.append(akey)\n",
    "\n",
    "# len(Xforpca_vgg) 2237\n",
    "# four_sec_chunks = Xforpca_vgg\n",
    "# this data will be used for training the last layer of the vggish network\n",
    "Xforpca_vgg_train = np.array(Xforpca_vgg_train).reshape((-1, 128))\n",
    "# print(Xforpca_vgg.shape) # (4*2237), 128\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pcadata_vgg_train = pca.fit_transform(Xforpca_vgg_train) # shape (nsamples, nfeatures)\n",
    "\n",
    "assert len(binarylabels_serial_train) == Xforpca_vgg_train.shape[0], \"len(binarylabels_serial) != Xforpca.shape[0]. %i != %i\" % (len(binarylabels_serial), Xforpca.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for test set\n",
    "\n",
    "binarylabels_serial_test = []\n",
    "Xforpca_vgg_test = []\n",
    "keys = []\n",
    "for akey in melspecs_test.keys():\n",
    "        if akey in embeddings:\n",
    "                if embeddings[akey].shape != (4,128):\n",
    "#                         print(\"  warning: skipping item %s because embedding is unexpected shape\" % akey)\n",
    "                        continue # goes to the beginning of the loop\n",
    "                binarylabels_serial_test.extend([files_Dictionary[akey]] * embeddings[akey].shape[0])\n",
    "                #print(embeddings[akey].shape[1])\n",
    "                Xforpca_vgg_test.append(embeddings[akey])\n",
    "                #print(Xforpca_vgg[0].shape)\n",
    "                keys.append(akey)\n",
    "\n",
    "# this data will be used for training the last layer of the vggish network\n",
    "Xforpca_vgg_test = np.array(Xforpca_vgg_test).reshape((-1, 128))\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pcadata_vgg_test = pca.fit_transform(Xforpca_vgg_test) # shape (nsamples, nfeatures)\n",
    "\n",
    "assert len(binarylabels_serial_test) == Xforpca_vgg_test.shape[0], \"len(binarylabels_serial) != Xforpca.shape[0]. %i != %i\" % (len(binarylabels_serial), Xforpca.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot data\n",
    "def scatterplot(xs, ys, title, datalabels):\n",
    "    plt.figure(frameon=False, figsize=(5, 5))\n",
    "    plt.scatter(xs, ys, alpha=0.2,\n",
    "                c=[{0: 'b', 1: 'g'}[albl] for albl in datalabels],\n",
    "               )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT BELOW FOR PLOTS\n",
    "# # Plotting PCA Analysis for both test and train\n",
    "# scatterplot(pcadata_vgg_train[:,0], pcadata_vgg_train[:,1], \"2D PCA of VGG train embeddings\", binarylabels_serial_train)\n",
    "# scatterplot(pcadata_vgg_test[:,0], pcadata_vgg_test[:,1], \"2D PCA of VGG test embeddings\", binarylabels_serial_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: Logistic regression applied to the VGG embeddings\n",
    "lgr = LogisticRegression(solver='liblinear')\n",
    "lgr.fit(Xforpca_vgg_train, binarylabels_serial_train)\n",
    "lgrdata_vgg = lgr.predict_proba(Xforpca_vgg_train) # shape (nsamples, nfeatures)\n",
    "lgrscore_vgg = lgr.score(Xforpca_vgg_train, binarylabels_serial_train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT BELOW FOR PLOTS\n",
    "# scatterplot(lgrdata_vgg[:,0], range(len(lgrdata_vgg)), \"Training accuracy: logistic regression of VGG embeddings (%.1f %%)\" % lgrscore_vgg, binarylabels_serial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing phase\n",
    "lgrdata_vgg = lgr.predict_proba(Xforpca_vgg_test) # shape (nsamples, nfeatures)\n",
    "lgrscore_vgg = lgr.score(Xforpca_vgg_test, binarylabels_serial_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT BELOW FOR PLOTS\n",
    "scatterplot(lgrdata_vgg[:,0], range(len(lgrdata_vgg)), \"Test Accuracy: Logistic regression of VGG embeddings (%.1f %%)\" % lgrscore_vgg, binarylabels_serial_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract classification probabilities for each example\n",
    "examples_prob = []\n",
    "for i in range(0, len(lgrdata_vgg), 4):\n",
    "    examples_prob.append(lgrdata_vgg[i:i+4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classifications based on probabilites, for each example\n",
    "classifications = []\n",
    "# Keys that were classified as music\n",
    "music_keys = []\n",
    "# Keys that were classified as non-music\n",
    "non_music_keys = []\n",
    "\n",
    "for i, prob in enumerate(examples_prob):\n",
    "    # first column -> probability of non music \n",
    "    no_music_prob = sum(prob[:,0])/len(prob)\n",
    "    # second column -> probability of music\n",
    "    music_prob = sum(prob[:,1])/len(prob)\n",
    "\n",
    "    if (no_music_prob >= music_prob):\n",
    "        output_class = 0\n",
    "        non_music_keys.append(keys[i])\n",
    "    else:\n",
    "        output_class = 1\n",
    "        music_keys.append(keys[i])\n",
    "    #print(output_class)\n",
    "    classifications.append(output_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save music_keys into pickle\n",
    "music_keys_pickle = open(\"music_keys.pickle\", \"wb\")\n",
    "pickle.dump(music_keys, music_keys_pickle)\n",
    "music_keys_pickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of networkOne on test audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file test case - Waving by October Horse \n",
    "audiotest_path = '/homes/pps30/venvs/venv_dl4am_a1/dl4am/assignment/Waving_OH.wav'\n",
    "y, sr = librosa.load(audiotest_path, mono=True)\n",
    "\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mel spectograms in vggish format\n",
    "audiotest_melspecs_vgg = vggish_input.waveform_to_examples(librosa.load(audiotest_path, mono=True)[0], librosa.load(audiotest_path, mono=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate embeddings from data\n",
    "embedding_audiotest = embedding_model.forward(audiotest_melspecs_vgg).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to check if it is music or non-music\n",
    "Xforpca_vgg_audiotest = embedding_audiotest.reshape((-1, 128))\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "audiotest_preds = lgr.predict_proba(Xforpca_vgg_audiotest) # shape (nsamples, nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of seconds as music:  169\n",
      "Number of seconds as non-music:  59\n",
      "Probability of being music:  72.3991263201452\n",
      "Probability of being non-music:  27.60087367985476\n"
     ]
    }
   ],
   "source": [
    "# Count how many seconds are considered music agaisnt how many are not\n",
    "# Calculate average probabilities for music and non-music\n",
    "\n",
    "counter_nomusic = 0\n",
    "counter_yesmusic = 0\n",
    "probs_nomusic = 0\n",
    "probs_yesmusic = 0 \n",
    "\n",
    "for i, prob in enumerate(audiotest_preds):\n",
    "    probs_nomusic += prob[0]\n",
    "    probs_yesmusic += prob[1]\n",
    "    \n",
    "    if prob[0] > prob[1]:\n",
    "        counter_nomusic += 1\n",
    "    else:\n",
    "        counter_yesmusic += 1        \n",
    "        \n",
    "avgProb_nomusic = probs_nomusic / len(audiotest_preds) * 100\n",
    "avgProb_yesmusic = probs_yesmusic / len(audiotest_preds) * 100\n",
    "\n",
    "print(\"Number of seconds as music: \", counter_yesmusic)\n",
    "print(\"Number of seconds as non-music: \", counter_nomusic)\n",
    "print(\"Probability of being music: \", avgProb_yesmusic)\n",
    "print(\"Probability of being non-music: \", avgProb_nomusic)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
